# Load Files into One Lake Table
## Create Workspace
1. Go to app.powerbi.com > Click on **Workspace** > Create **New Workspace**
![image](https://github.com/rritec/dataFabric/assets/20516321/fc06467c-8cc8-4b8f-a098-28e11af105f7)



## Create OneLake
1. Click on **New** > Click on **More Options** > Click on **Lake House** > Name it as **rritecOneLake**
![image](https://github.com/rritec/dataFabric/assets/20516321/263d3de2-44ab-486b-af20-d970bf19d9fa)

## create folders and upload files
1. Click on **rritecOneLake** **>** Click on **Files ...** **>** Clcik on **New Sub Folder** **>** Name it as **study**
2. Click on **study ...** **>** Clcik on **New Sub Folder** **>>** Name it as **nbu**
2. Click on **study ...** > Clcik on **New Sub Folder** > Name it as **obu**
3. Click on **nbu ...** > Clcik on **upload** > Click on **upload Files** > Click on ![image](https://github.com/rritec/dataFabric/assets/20516321/c2b6c690-6eb4-4921-b7ca-0430a727f12d) > Select two Files emp.csv and dept.csv from **labdata** folder > click on **open** > click on **upload**
4. Click on **obu ...** > Clcik on **upload** > Click on **upload Files** > Click on ![image](https://github.com/rritec/dataFabric/assets/20516321/c2b6c690-6eb4-4921-b7ca-0430a727f12d) > Select two Files emp.csv and dept.csv from **labdata** folder > click on **open** > click on **upload**
![image](https://github.com/rritec/dataFabric/assets/20516321/eb948d3e-c404-4751-9251-cdcd69a4d64a)
![image](https://github.com/rritec/dataFabric/assets/20516321/317b4d55-6504-479c-8741-45dc75303c46)


## Create Notebook
1. Go Back to **rritecWorkSpace**
2. Click on **New** **>>** Click on **More Options** **>>** Click on **Notebook** **>>** Name it as **rritecNotebook**
3. Click on **Lakehouse** **>>** Click on **Add** **>>** Select **Existing Lakehouse** >> Click on **Add** >> Select **rritecOnelake** >> click on **Add**
4. Type below code
``` py
def get_required_file_recursively(file_path,file_name):
    list_of_files=[]
    for i in mssparkutils.fs.ls(file_path):
        #print(i.path)
        for i in mssparkutils.fs.ls(i.path):
            if i.name ==file_name:
                #print(i.path,i.name)
                list_of_files.append(i.path)
    return list_of_files
```
``` py
file_path="abfss://rritecWorkspace@onelake.dfs.fabric.microsoft.com/rritecOneLake.Lakehouse/Files/study"
file_name="emp.csv"
```
``` py
# Create a Delta Lake table
table_name = "my_csv_table"
for i in get_required_file_recursively(file_path,file_name):
    df = spark.read.option("header", "true").csv(i)
    df.write.format("delta").mode("append").option("overwriteSchema", "true").saveAsTable(table_name)
```
``` py
df = spark.sql("SELECT * FROM rritecOneLake.my_csv_table LIMIT 1000")
display(df)

```

## Create pipeline

Go to rritecworkspace > open notebook


![image](https://github.com/rritec/dataFabric/assets/20516321/58ac8519-1662-46b0-af8b-fd9a81257094)

In **home** click on pipeline symbol(beside data wrangling)

![image](https://github.com/rritec/dataFabric/assets/20516321/07cf1897-9acf-477a-a038-196f8fe42642)

click on new pipeline > name the pipeline **pipeline_1** > click on settings. check**workspace** and **notebook**

![image](https://github.com/rritec/dataFabric/assets/20516321/baaa9755-73c4-465f-9ebf-cdc3f9bed132)

run the pipeline

![image](https://github.com/rritec/dataFabric/assets/20516321/1711f6a5-54a7-4587-9948-1ba7b200acad)






